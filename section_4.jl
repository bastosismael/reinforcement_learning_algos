using Statistics
# Monte Carlo Exploring Starts( Estimating Ï€ optimal)
struct State
    x::Int
    y::Int
end

# Definition of the environment
n = 4
ğ’® = [State(x,y) for x=1:n, y=1:n]
terminal_states = [State(1,4), State(1,1)]
inbounds(s::State) = 1 â‰¤ s.x â‰¤ n && 1 â‰¤ s.y â‰¤ n 
Base.:+(s1::State, s2::State) = State(s1.x + s2.x, s1.y +s2.y)
Base.:*(s1::State, b::Bool) = State(s1.x*b, s1.y*b)
isterminal(s) = s âˆˆ terminal_states

@enum Actions UP DOWN LEFT RIGHT STAY
const ğ’œ = Dict(UP => State(0, 1), DOWN => State(0, -1), RIGHT => State(1, 0), LEFT => State(-1, 0), STAY => State(0,0))

function R(s)
    if s âˆˆ terminal_states
        return 10
    else
        return -1
    end
end

function Ï€â‚€(s)
    return rand(ğ’œ)[1]
end


function generate_episode(ğ’®, ğ’œ, Ï€::Dict, Ï€â‚’::Function)
    sâ‚€ = rand(ğ’®)
    aâ‚€ = rand(ğ’œ)[1]
    episode = [(sâ‚€, aâ‚€)]
    s = sâ‚€ +  (ğ’œ[aâ‚€] *  inbounds(sâ‚€ + ğ’œ[aâ‚€]))
    while ~isterminal(s)
        a = get(Ï€, s, Ï€â‚€(s))
        if ~inbounds(s + ğ’œ[a])
            a = STAY
        end
        s = s + ğ’œ[a]
        push!(episode, (s, a))
    end
    return episode
end

function monte_carlo_es(ğ’®, ğ’œ, Î³=1)
    # Initializaing Ï€ and Q
    Ï€áµ§ = Dict{State, Actions}()
    Q = Dict(s => Dict(a => 0.0 for a âˆˆ keys(ğ’œ)) for s in ğ’®)
    returns = Dict((s,a) => 0.0 for s in ğ’®, a âˆˆ keys(ğ’œ))
    for i in 1:3
        episode = generate_episode(ğ’®, ğ’œ, Ï€áµ§, Ï€â‚€)
        G = 0
        for t = reverse(1:length(episode))
            print(t)
            sâ‚œ, aâ‚œ = episode[t][1], episode[t][2] 
            G = Î³*G + R(sâ‚œ)
            if (sâ‚œ, aâ‚œ) âˆ‰ episode[1:t-1]
                returns[(sâ‚œ, aâ‚œ)] = G
                   Q[sâ‚œ][aâ‚œ] = mean(returns[(sâ‚œ, aâ‚œ)])
                 Ï€áµ§[sâ‚œ] = findmax(Q[sâ‚œ])[2]
            end
         end
    end
    return Ï€áµ§
end


#Ï€ = Dict(s => rand(ğ’œ) for s in ğ’®)

# A = generate_episode(ğ’®, ğ’œ, Dict{State, Actions}(), Ï€â‚€)
mt = monte_carlo_es(ğ’®, ğ’œ)