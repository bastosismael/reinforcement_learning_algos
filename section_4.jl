using Statistics
using StatsBase
using Distributions

# Monte Carlo Exploring Starts( Estimating Ï€ optimal)
struct State
    x::Int
    y::Int
end

# Definition of the environment
n = 4
ğ’® = [State(x,y) for x=1:n, y=1:n]
terminal_states = [State(2,2), State(3,3)]
inbounds(s::State) = 1 â‰¤ s.x â‰¤ n && 1 â‰¤ s.y â‰¤ n 
Base.:+(s1::State, s2::State) = State(s1.x + s2.x, s1.y +s2.y)
Base.:*(s1::State, b::Bool) = State(s1.x*b, s1.y*b)
isterminal(s) = s âˆˆ terminal_states

@enum Actions UP DOWN LEFT RIGHT STAY
const ğ’œ = Dict(UP => State(0, 1), DOWN => State(0, -1), RIGHT => State(1, 0), LEFT => State(-1, 0), STAY => State(0,0))

function R(s)
    if s âˆˆ terminal_states
        return 10
    else
        return -1
    end
end

function Ï€â‚€(s)
    return rand(ğ’œ)[1]
end


# This code is hand-made just for the monte carlos ES (The worst algorithm ever)
function generate_episode_es(ğ’®, ğ’œ, Ï€::Dict, Ï€â‚’::Function)
    sâ‚€ = rand(ğ’®)
    aâ‚€ = rand(ğ’œ)[1]
    if ~inbounds(sâ‚€ + ğ’œ[aâ‚€])
        aâ‚€ = STAY
        episode = [(sâ‚€, aâ‚€)]
    end
    episode = [(sâ‚€, aâ‚€)]
    if isterminal(sâ‚€)
        return episode
    end
    s = sâ‚€ +  ğ’œ[aâ‚€]
    while ~isterminal(s)
        a = get(Ï€, s, Ï€â‚€(s))
        if ~inbounds(s + ğ’œ[a])
            a = STAY
        end
        push!(episode, (s, a))
        s = s + ğ’œ[a]   
    end
    push!(episode, (s, STAY))
    return episode
end



# Problem: The montecarlo ES has the problem that
# if the Ï€ is not irreducible, so the function ends 
# in a eternal loop. 

function monte_carlo_es(ğ’®, ğ’œ, Î³=1)
    # Initializaing Ï€ and Q
    Ï€áµ§ = Dict{State, Actions}()
    Q = Dict{State, Dict{Actions, Float64}}()
    returns = Dict((s,a) => [] for s in ğ’®, a âˆˆ keys(ğ’œ))
    for i in 1:5
        println(i)
        episode = generate_episode_es(ğ’®, ğ’œ, Ï€áµ§, Ï€â‚€)
        G = 0
        for t = reverse(1:length(episode)-1)
            println(t)
            sâ‚œ, aâ‚œ = episode[t][1], episode[t][2] 
            sâ‚Š = episode[t+1][1]
            G = Î³*G + R(sâ‚Š)
            if (sâ‚œ, aâ‚œ) âˆ‰ episode[1:t-1]
                push!(returns[(sâ‚œ, aâ‚œ)], G)
                Q[sâ‚œ] = Dict(aâ‚œ => mean(returns[(sâ‚œ, aâ‚œ)]))
                Ï€áµ§[sâ‚œ] = findmax(Q[sâ‚œ])[2]
            end
         end
    end
    return Ï€áµ§
end

function generate_episode(ğ’®, ğ’œ, Ï€::Dict)
    sâ‚€ = rand(ğ’®)
    aâ‚€ = rand(ğ’œ)[1]
    if ~inbounds(sâ‚€ + ğ’œ[aâ‚€])
        aâ‚€ = STAY
        episode = [(sâ‚€, aâ‚€)]
    end
    episode = [(sâ‚€, aâ‚€)]
    if isterminal(sâ‚€)
        return episode
    end
    s = sâ‚€ +  ğ’œ[aâ‚€]
    while ~isterminal(s)
        a = sample(collect(keys(Ï€[s])), Weights(collect(values(Ï€[s]))), 1)[1]
        if ~inbounds(s + ğ’œ[a])
            a = STAY
        end
        push!(episode, (s, a))
        s = s + ğ’œ[a]   
    end
    push!(episode, (s, STAY))
    return episode
end

function monte_carlo_control(Ïµ, Î³=1, n=100)
    # The comments are the case when we store the reutrns, less efficient
    Ï€ = Dict(s => Dict(a => ifelse(s âˆ‰ terminal_states, 1/length(ğ’œ), ifelse(a == STAY, 1.0, 0.0)) for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
    # Q = Dict{State, Dict{Actions, Float64}}()
    # returns = Dict((s,a) => [] for s âˆˆ ğ’®, a âˆˆ keys(ğ’œ))
    C = Dict((s,a) => 0 for s âˆˆ ğ’®, a âˆˆ keys(ğ’œ))
    Q = Dict{State, Dict{Actions, Float64}}(s => Dict(a => 0.0 for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
    for i=1:n
        println(i)
        episode = generate_episode(ğ’®, ğ’œ, Ï€)
        G = 0
        for t=reverse(1:length(episode)-1)
            sâ‚œ, aâ‚œ = episode[t][1], episode[t][2] 
            sâ‚Š = episode[t+1][1]
            G = Î³*G + R(sâ‚Š)
            if (sâ‚œ, aâ‚œ) âˆ‰ episode[1:t-1]
                # push!(returns[(sâ‚œ, aâ‚œ)], G)
                # Q[sâ‚œ]  = Dict(aâ‚œ => mean(returns[(sâ‚œ, aâ‚œ)]))
                C[sâ‚œ,aâ‚œ] += 1
                Q[sâ‚œ][aâ‚œ] = Q[sâ‚œ][aâ‚œ] + 1/C[sâ‚œ,aâ‚œ] * (G - Q[sâ‚œ][aâ‚œ])
                A = findmax(Q[sâ‚œ])[2]
                for a âˆˆ keys(ğ’œ)
                    Ï€[sâ‚œ][a] = ifelse(a == A, 1 - Ïµ + Ïµ/length(ğ’œ), Ïµ/length(ğ’œ))
                end
            end
        end
    end
    return(Ï€)
end

function off_policy_mc(Î³=1, n=500)
    Q = Dict(s => Dict(a => rand(Uniform(0,10)) for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
    C = Dict(s => Dict(a => 0.0 for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
    Ï€ = Dict(s => findmax(Q[s])[2] for s âˆˆ ğ’®)

    for i=1:n
        b = Dict(s => Dict(a => ifelse(s âˆ‰ terminal_states, 1/length(ğ’œ), ifelse(a==STAY, 1.0, 0.0)) for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
        episode = generate_episode(ğ’®, ğ’œ, b)
        G = 0
        W = 1
        for t=reverse(1:length(episode) -1)
            sâ‚œ, aâ‚œ = episode[t][1], episode[t][2]
            sâ‚Š = episode[t+1][1]
            G = Î³*G + R(sâ‚Š)
            C[sâ‚œ][aâ‚œ] = C[sâ‚œ][aâ‚œ] + W
            Q[sâ‚œ][aâ‚œ] = Q[sâ‚œ][aâ‚œ] + W/C[sâ‚œ][aâ‚œ] * (G - Q[sâ‚œ][aâ‚œ])
            Ï€[sâ‚œ] = findmax(Q[sâ‚œ])[2]
            if aâ‚œ â‰  Ï€[sâ‚œ] 
                break
            end
            W = W * 1/b[sâ‚œ][aâ‚œ]
        end
    end
    return Ï€
end


function sarsa(Î±, Ïµ, Î³=1)
    Q = Dict(s => Dict(a => -200.0 for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
    Ï€ = Dict(s => Dict(a => ifelse(s âˆ‰ terminal_states, 1/length(ğ’œ), ifelse(a == STAY, 1.0, 0.0)) for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
    for i=1:10000
        episode = generate_episode(ğ’®, ğ’œ, Ï€)
        for t=1:length(episode)-1
            s, a = episode[t][1], episode[t][2]
            sâ€², aâ€² = episode[t+1][1], episode[t+1][2]
            r = R(sâ€²)
            Q[s][a] = Q[s][a] + Î±*(r + Î³*Q[sâ€²][aâ€²] - Q[s][a])
            s = sâ€²
            a = aâ€²
        end
        Ï€ = Dict(s => Dict(a => ifelse(a == findmax(Q[s])[2], 1 - Ïµ + Ïµ/length(ğ’œ), Ïµ/length(ğ’œ)) for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
    end
    return Ï€
end

function q_learning(Î±, Ïµ, Î³=1)
    Q = Dict(s => Dict(a => -100.0 for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
    Ï€ = Dict(s => Dict(a => ifelse(s âˆ‰ terminal_states, 1/length(ğ’œ), ifelse(a == STAY, 1.0, 0.0)) for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
    for i=1:8000
        episode = generate_episode(ğ’®, ğ’œ, Ï€)
        for t=1:length(episode)-1
            s, a = episode[t][1], episode[t][2]
            sâ€² = episode[t+1][1]
            r = R(sâ€²)
            Q[s][a] = Q[s][a] + Î±*(r + Î³*findmax(Q[sâ€²])[1] - Q[s][a])
        end
        Ï€ = Dict(s => Dict(a => ifelse(a == findmax(Q[s])[2], 1 - Ïµ + Ïµ/length(ğ’œ), Ïµ/length(ğ’œ)) for a âˆˆ keys(ğ’œ)) for s âˆˆ ğ’®)
    end
    return Ï€
end


#Ï€ = Dict(s => rand(ğ’œ) for s in ğ’®)

# A = generate_episode(ğ’®, ğ’œ, Dict{State, Actions}(), Ï€â‚€)
#mt = monte_carlo_es(ğ’®, ğ’œ)
#m = monte_carlo_control(1e-2, 1, 1000)
#ms = monte_carlo_es(ğ’®, ğ’œ)
# off_mc = off_policy_mc()
#print(m[State(1,2)])
s =  sarsa(0.5, 0.01)
d = q_learning(0.5, 0.01)

