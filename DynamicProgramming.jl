using Parameters
using Statistics

struct State
	x::Int
	y::Int
end

@with_kw struct params
	N::Int = 4
	null_state::Array{State} = [State(1,1), State(4,4)]
	p::Real = 0.5
end

par = params()
ğ’® = [State(i,j) for i=1:par.N for j=1:par.N]
@enum Action LEFT RIGHT UP DOWN
ğ’œ = [LEFT, RIGHT, UP, DOWN]

inbounds(s::State) = 1 â‰¤ s.x â‰¤ par.N && 1 â‰¤ s.y â‰¤ par.N

const MOVEMENTS = Dict(LEFT => State(-1,0),
						RIGHT => State(1,0),
						UP => State(0,1),
						DOWN => State(0,-1));
Base.:+(s1::State, s2::State) = State(s1.x + s2.x, s1.y + s2.y)

function T(s::State, a::Action, sâ€²::State)
	return 1
end

function R(s::State)
	if s âˆˆ par.null_state
		return -1
	else
		return -1
	end
end

struct MDP
	Î³::Real
	ğ’®::Array{State}
	ğ’œ::Array{Action}
	T
	R
end

function Ï€(a, s)
	if s âˆˆ par.null_state
		return 0
	else
		return 1/length(ğ’œ)
	end
end

Î³ = 1
ğ’« = MDP(Î³, ğ’®, ğ’œ, T, R)

function lookahead(ğ’«::MDP, ğ’±::Dict, s)
	ğ’®, ğ’œ, T, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
	value = 0
	for a âˆˆ ğ’œ
		if ~inbounds(s + MOVEMENTS[a])
			sâ€² = s
		else
			sâ€² = s + MOVEMENTS[a]
		end
		value += Ï€(a, s) * T(s,a,sâ€²) * (Î³*ğ’±[sâ€²] + R(sâ€²))
	end
	return value
end

function iterative_policy_evaluation(ğ’«::MDP, Ï€, k_max=true)
	ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
	ğ’± = Dict(s => 0 for s in ğ’®)
	for k in 1:k_max
		ğ’± = Dict(ifelse(s âˆ‰  par.null_state, s => lookahead(ğ’«, ğ’±, s), s => 0) for s in ğ’®)
	end
	return ğ’±
end


function policy_iteration(ğ’«::MDP, Ï€, Î¸)
	ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
	ğ’± = Dict(s => 0 for s in ğ’®)
	Î” = 1
	while Î” > Î¸ 
		ğ’±â‚ = ğ’±
		ğ’± = Dict(ifelse(s âˆ‰  par.null_state, s => lookahead(ğ’«, ğ’±, s), s => 0) for s in ğ’®)
		Î” = maximum([abs(x) for x âˆˆ collect(values(ğ’±)) - collect(values(ğ’±â‚))])
	end
	return ğ’±
end

function return_valid_state(s, a)
	if ~inbounds(s + MOVEMENTS[a])
		sâ€² = s
	else
		sâ€² = s + MOVEMENTS[a]
	end
	return sâ€²
end




function policy_improvement(ğ’«::MDP, Ï€)
	ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
	Ï€â‚€ = Dict(s => UP for s âˆˆ ğ’®)
	# Policy Evaluation
	ğ’± = policy_iteration(ğ’«, Ï€, 0.01)
	#Policy Improvement
	policy_stable = false
	Ï€â‚ = Ï€â‚€
	while ~policy_stable
		for s âˆˆ ğ’® 
			action_values = Dict(a => T(s, a, return_valid_state(s,a))*(R(return_valid_state(s,a)) + Î³*ğ’±[return_valid_state(s,a)]) for a âˆˆ ğ’œ)
			v, aâ‚€ = findmax(action_values)
			Ï€â‚[s] = aâ‚€
		end
		if Ï€â‚€ == Ï€â‚
			policy_stable = true
		else
			Ï€â‚€ = Ï€â‚
		end
	end
	return Ï€â‚€
end


function gen_episode(ğ’«::MDP)
	ğ’œ, ğ’®, R = ğ’«.ğ’œ, ğ’«.ğ’®, ğ’«.R
	sâ‚€ = rand(ğ’®)
	trajectory = [sâ‚€]
	s = sâ‚€
	while s âˆ‰ par.null_state
		s = rand(Set([ifelse(inbounds(s+MOVEMENTS[a]), s+MOVEMENTS[a], s) for a âˆˆ ğ’œ]) )
		push!(trajectory, s)
	end
	return trajectory
end
function mc_policy_evaluation(Ï€, k_max=100)
	ğ’± = Dict(s => 0.0 for s âˆˆ ğ’®)
	returns = Dict(s => [] for s âˆˆ ğ’®)
	for i in 1:k_max
		episode = gen_episode(ğ’«)
		G = 0
		for i in reverse(1:length(episode))
			G = Î³*G + R(episode[i])
			if episode[i] âˆ‰ episode[1:i-1] && episode[i] âˆ‰ par.null_state
				push!(returns[episode[i]], G)
				ğ’±[episode[i]] = mean(returns[episode[i]])
			end
		end
	end
	return ğ’±
end
		




# ğ’±â‚€ = Dict(s => 0 for s in ğ’®)
# lookahead(ğ’«, ğ’±â‚€, State(2,2))

gen_episode(ğ’«)
policy_iteration(ğ’«, Ï€, 0.0001)
policy_improvement(ğ’«, Ï€)
mc_policy_evaluation(ğ’«, 20000)